{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy37gQx/FWt3FoROZNji7w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meiyee1010/gdp-dashboard/blob/main/To_achieve_RO_Include_structured_noise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W1XqszsVZwS5",
        "outputId": "23718ecf-3cb2-4203-d4c6-28918ffacc80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m258.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m214.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m214.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m146.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m213.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m233.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m321.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m296.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m235.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m315.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m232.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m239.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m205.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m219.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m252.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tqdm, threadpoolctl, regex, numpy, joblib, click, smart-open, scipy, nltk, scikit-learn, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.0\n",
            "    Uninstalling joblib-1.5.0:\n",
            "      Successfully uninstalled joblib-1.5.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.0\n",
            "    Uninstalling click-8.2.0:\n",
            "      Successfully uninstalled click-8.2.0\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-8.2.0 gensim-4.3.3 joblib-1.5.0 nltk-3.9.1 numpy-1.26.4 regex-2024.11.6 scikit-learn-1.6.1 scipy-1.13.1 smart-open-7.1.0 threadpoolctl-3.6.0 tqdm-4.67.1 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim",
                  "joblib",
                  "nltk",
                  "regex",
                  "smart_open",
                  "wrapt"
                ]
              },
              "id": "4b28a42f20d64427a5635a00ac878713"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# 0) Force-reinstall NumPy, Gensim, scikit-learn, and NLTK to fix the dtype mismatch\n",
        "!pip install --upgrade --force-reinstall --no-cache-dir numpy\n",
        "!pip install --upgrade --force-reinstall --no-cache-dir gensim scikit-learn nltk\n",
        "\n",
        "# 1) Imports (after reinstall)\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
        "import numpy as np\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "# 2) Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "rT1foJsxHmPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 1) Preprocessing\n",
        "# ========================\n",
        "def preprocess_texts(texts):\n",
        "    \"\"\"Tokenize and remove stopwords.\"\"\"\n",
        "    tok = []\n",
        "    for doc in texts:\n",
        "        tokens = simple_preprocess(doc, deacc=True)\n",
        "        tokens = [w for w in tokens if w not in stop_words]\n",
        "        tok.append(tokens)\n",
        "    return tok\n",
        "\n",
        "def build_corpus(texts_tokens):\n",
        "    \"\"\"Build Gensim dictionary & corpus from token lists.\"\"\"\n",
        "    dictionary = Dictionary(texts_tokens)\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in texts_tokens]\n",
        "    return dictionary, corpus\n"
      ],
      "metadata": {
        "id": "hTKN1dLiJO4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 2) Baseline LDA Runner\n",
        "# ========================\n",
        "from gensim.models import LdaMulticore\n",
        "\n",
        "def run_baseline_lda(dictionary, corpus, texts_tokens, labels, num_topics=4, passes=10, iterations=50):\n",
        "    tracemalloc.start()\n",
        "    start = time.time()\n",
        "    model = LdaMulticore(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        passes=passes,\n",
        "        iterations=iterations,\n",
        "        chunksize=2000,\n",
        "        workers=4,\n",
        "        alpha='symmetric',\n",
        "        random_state=42,\n",
        "        eval_every=None\n",
        "    )\n",
        "    runtime = time.time() - start\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    # Assign topics to docs\n",
        "    assigned = [\n",
        "        max(model.get_document_topics(doc), key=lambda x: x[1])[0]\n",
        "        for doc in corpus\n",
        "    ]\n",
        "\n",
        "# Coherence\n",
        "    coh = CoherenceModel(model=model,\n",
        "                         texts=texts_tokens,\n",
        "                         dictionary=dictionary,\n",
        "                         coherence='c_v').get_coherence()\n",
        "\n",
        "# Clustering metrics\n",
        "    hom = homogeneity_score(labels, assigned)\n",
        "    com = completeness_score(labels, assigned)\n",
        "    vm  = v_measure_score(labels, assigned)\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'coherence': coh,\n",
        "        'homogeneity': hom,\n",
        "        'completeness': com,\n",
        "        'v_measure': vm,\n",
        "        'runtime_s': runtime,\n",
        "        'mem_peak_mb': peak / 1e6\n",
        "    }"
      ],
      "metadata": {
        "id": "rfRpoP44JTQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 3) Hybrid LDA Runner\n",
        "# ========================\n",
        "def run_hybrid_lda(dictionary,\n",
        "                   corpus,\n",
        "                   texts_tokens,\n",
        "                   labels,\n",
        "                   num_topics=4,\n",
        "                   emb_size=100,\n",
        "                   base_eta=0.01,\n",
        "                   high_eta=0.1,\n",
        "                   passes=5,\n",
        "                   iterations=30):\n",
        "    \"\"\"\n",
        "    1) Train Word2Vec on texts_tokens\n",
        "    2) KMeans cluster the resulting embeddings\n",
        "    3) Build a (num_topics x vocab_size) eta matrix\n",
        "    4) Train LdaMulticore with that eta\n",
        "    5) Compute and return all metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # --- a) Train Word2Vec ---\n",
        "    w2v = Word2Vec(\n",
        "    sentences=texts_tokens,\n",
        "    vector_size=emb_size,\n",
        "    window=5,\n",
        "    min_count=1,    # ← was 2\n",
        "    workers=4,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "    # --- b) Extract word vectors for every token in your dictionary ---\n",
        "    vocab_size = len(dictionary)\n",
        "    word_vecs = np.vstack([w2v.wv[dictionary[i]] for i in range(vocab_size)])\n",
        "\n",
        "    # --- c) Cluster those vectors into num_topics clusters ---\n",
        "    kmeans = KMeans(n_clusters=num_topics, random_state=0, n_init=10)\n",
        "    kmeans.fit(word_vecs)\n",
        "    word_to_topic = kmeans.labels_   # array of length vocab_size\n",
        "\n",
        "    # --- d) Build eta matrix (guaranteed to exist now) ---\n",
        "    eta = np.full((num_topics, vocab_size), base_eta, dtype=float)\n",
        "    for word_id, topic_id in enumerate(word_to_topic):\n",
        "        eta[topic_id, word_id] = high_eta\n",
        "\n",
        "    # --- e) Train LdaMulticore with that custom eta ---\n",
        "    tracemalloc.start()\n",
        "    t0 = time.time()\n",
        "    model = LdaMulticore(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        passes=passes,\n",
        "        iterations=iterations,\n",
        "        chunksize=len(corpus),\n",
        "        workers=4,\n",
        "        alpha='symmetric',  # multicore does NOT support 'auto'\n",
        "        eta=eta,\n",
        "        random_state=42,\n",
        "        eval_every=None\n",
        "    )\n",
        "    runtime = time.time() - t0\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    # --- f) Assign each document to its top topic ---\n",
        "    assigned = [\n",
        "        max(model.get_document_topics(doc), key=lambda x: x[1])[0]\n",
        "        for doc in corpus\n",
        "    ]\n",
        "\n",
        "    # --- g) Compute coherence and clustering metrics ---\n",
        "    coherence = CoherenceModel(\n",
        "        model=model,\n",
        "        texts=texts_tokens,\n",
        "        dictionary=dictionary,\n",
        "        coherence='c_v'\n",
        "    ).get_coherence()\n",
        "    homogeneity   = homogeneity_score(labels, assigned)\n",
        "    completeness  = completeness_score(labels, assigned)\n",
        "    v_measure     = v_measure_score(labels, assigned)\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'coherence': coherence,\n",
        "        'homogeneity': homogeneity,\n",
        "        'completeness': completeness,\n",
        "        'v_measure': v_measure,\n",
        "        'runtime_s': runtime,\n",
        "        'mem_peak_mb': peak / 1e6\n",
        "    }"
      ],
      "metadata": {
        "id": "tYWI5gl5Jie5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 4) Load & Prepare Datasets\n",
        "# ========================\n",
        "# 4a) Synthetic (we generated earlier)\n",
        "\n",
        "\n",
        "# --- 1) Configuration & Templates ---\n",
        "\n",
        "categories = {\n",
        "    \"work\": [\n",
        "        \"Meeting scheduled for next Monday.\",\n",
        "        \"Please send your project update.\",\n",
        "        \"Reminder: Submit your timesheet by Friday.\",\n",
        "        \"Team lunch on Wednesday at noon.\",\n",
        "        \"Follow up on Q2 marketing results.\"\n",
        "    ],\n",
        "    \"promotion\": [\n",
        "        \"Exclusive offer: 50% off on all items!\",\n",
        "        \"Summer sale starts now - don't miss it!\",\n",
        "        \"Buy one get one free - limited time only.\",\n",
        "        \"New arrivals in our store this week.\",\n",
        "        \"Free shipping on orders over $50.\"\n",
        "    ],\n",
        "    \"scam\": [\n",
        "        \"Your account has been suspended. Click here to verify.\",\n",
        "        \"Congratulations! You've won a lottery prize.\",\n",
        "        \"Urgent: Update your banking details now.\",\n",
        "        \"This is your final warning before account closure.\",\n",
        "        \"You've received a secure message - view now.\"\n",
        "    ],\n",
        "    \"news\": [\n",
        "        \"Local football team wins championship.\",\n",
        "        \"Weather alert: Heavy rain expected tomorrow.\",\n",
        "        \"Community meeting scheduled for next week.\",\n",
        "        \"Mayor announces new green initiative.\",\n",
        "        \"City library to host book fair this Saturday.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "TOTAL = 5000\n",
        "PER_CAT = TOTAL // len(categories)    # 1250 per label\n",
        "\n",
        "# Need to import random module\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# --- 2) Precompute which indices get structural noise (~5%) ---\n",
        "N_STRUCT = int(0.05 * TOTAL)        # 250\n",
        "struct_noise_idxs = set(random.sample(range(TOTAL), k=N_STRUCT))\n",
        "\n",
        "data = []\n",
        "idx = 0\n",
        "for label, templates in categories.items():\n",
        "    for _ in range(PER_CAT):\n",
        "        # a) Base subject & body\n",
        "        subj = random.choice(templates)\n",
        "        body = f\"{subj} Please read the details and act accordingly.\"\n",
        "\n",
        "        # b) 10% token-level noise\n",
        "        tokens = body.split()\n",
        "        n_tok_noise = int(0.1 * len(tokens))\n",
        "        for j in random.sample(range(len(tokens)), k=n_tok_noise):\n",
        "            tokens[j] = random.choice(tokens)  # could also choose from full vocab\n",
        "        text_noisy = \" \".join(tokens)\n",
        "\n",
        "        # c) **Structural noise** on exactly 5% of samples\n",
        "        if idx in struct_noise_idxs:\n",
        "            pos = random.randint(0, len(subj) - 1)\n",
        "            subj = subj[:pos] + random.choice(['#', '@', '%']) + subj[pos:]\n",
        "\n",
        "        data.append({\n",
        "            \"subject\": subj,\n",
        "            \"text\":    text_noisy,\n",
        "            \"label\":   label\n",
        "        })\n",
        "        idx += 1\n",
        "\n",
        "# --- 3) Save to CSV ---\n",
        "df_synth = pd.DataFrame(data)\n",
        "synth_path = \"/content/synthetic_email_dataset_5000.csv\"\n",
        "df_synth.to_csv(synth_path, index=False)\n",
        "print(f\"Saved synthetic CSV with {len(df_synth)} rows to {synth_path}\")\n",
        "\n",
        "# ========================\n",
        "# 4b) Load & Preprocess\n",
        "# ========================\n",
        "# Synthetic\n",
        "df_synth = pd.read_csv(synth_path)\n",
        "texts_s = df_synth['text'].tolist()\n",
        "tokens_s = preprocess_texts(texts_s)\n",
        "dict_s, corpus_s = build_corpus(tokens_s)\n",
        "labels_s = df_synth['label'].astype('category').cat.codes.tolist()\n",
        "\n",
        "# Real (Kaggle)\n",
        "df_kag = pd.read_csv('/content/spam_ham_dataset.csv')\n",
        "texts_k = df_kag['text'].tolist()\n",
        "tokens_k = preprocess_texts(texts_k)\n",
        "dict_k, corpus_k = build_corpus(tokens_k)\n",
        "labels_k = df_kag['label'].astype('category').cat.codes.tolist()\n",
        "\n",
        "print(\"Synthetic tokens example:\", tokens_s[0][:10])\n",
        "print(\"Kaggle tokens example:   \", tokens_k[0][:10])"
      ],
      "metadata": {
        "id": "WkBHEPQnJk-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc2f873-e8a0-4c7e-ca0d-b6535ec75f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved synthetic CSV with 5000 rows to /content/synthetic_email_dataset_5000.csv\n",
            "Synthetic tokens example: ['please', 'send', 'project', 'update', 'please', 'please', 'details', 'act', 'accordingly']\n",
            "Kaggle tokens example:    ['subject', 'enron', 'methanol', 'meter', 'follow', 'note', 'gave', 'monday', 'preliminary', 'flow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing & corpus creation\n",
        "tokens_s = preprocess_texts(df_synth['text'].tolist())\n",
        "dict_s, corpus_s = build_corpus(tokens_s)\n",
        "labels_s = df_synth['label'].astype('category').cat.codes.tolist()\n",
        "\n",
        "tokens_k = preprocess_texts(df_kag['text'].tolist())\n",
        "dict_k, corpus_k = build_corpus(tokens_k)\n",
        "labels_k = df_kag['label'].astype('category').cat.codes.tolist()\n"
      ],
      "metadata": {
        "id": "kLjz8pWifGmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 5) Run Experiments\n",
        "# ========================\n",
        "results = []\n",
        "\n",
        "for name, (dictionary, corpus, tokens, labels) in [\n",
        "    ('Synthetic', (dict_s, corpus_s, tokens_s, labels_s)),\n",
        "    ('Kaggle',    (dict_k, corpus_k, tokens_k, labels_k))\n",
        "]:\n",
        "    base = run_baseline_lda(\n",
        "        dictionary=dictionary,\n",
        "        corpus=corpus,\n",
        "        texts_tokens=tokens,\n",
        "        labels=labels,\n",
        "        num_topics=4,\n",
        "        passes=10,\n",
        "        iterations=100\n",
        "    )\n",
        "    hyb = run_hybrid_lda(\n",
        "        dictionary=dictionary,\n",
        "        corpus=corpus,\n",
        "        texts_tokens=tokens,\n",
        "        labels=labels,\n",
        "        num_topics=4,\n",
        "        emb_size=50,\n",
        "        base_eta=0.01,\n",
        "        high_eta=0.1,\n",
        "        passes=10,\n",
        "        iterations=100\n",
        "    )\n",
        "\n",
        "    for tag, res in [('Baseline', base), ('Hybrid', hyb)]:\n",
        "        results.append({\n",
        "            'Dataset':        name,\n",
        "            'Model':          tag,\n",
        "            'Coherence':      res['coherence'],\n",
        "            'Homogeneity':    res['homogeneity'],\n",
        "            'Completeness':   res['completeness'],\n",
        "            'V-measure':      res['v_measure'],\n",
        "            'Runtime (s)':    res['runtime_s'],\n",
        "            'Memory Peak (MB)': res['mem_peak_mb']\n",
        "        })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qteV1zGfMqdm",
        "outputId": "306e5b16-5da1-4346-b0a0-cf7b8f532ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x7fbe10136980>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n",
            "    self._make_controller_from_path(filepath)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1187, in _make_controller_from_path\n",
            "    lib_controller = controller_class(\n",
            "                     ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 114, in __init__\n",
            "    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OSError: dlopen() error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Dataset     Model  Coherence  Homogeneity  Completeness  V-measure  \\\n",
            "0  Synthetic  Baseline   0.309325     0.113982      0.165399   0.134959   \n",
            "1  Synthetic    Hybrid   0.341003     0.277661      0.292826   0.285042   \n",
            "2     Kaggle  Baseline   0.489284     0.441085      0.227000   0.299741   \n",
            "3     Kaggle    Hybrid   0.506551     0.576864      0.337584   0.425918   \n",
            "\n",
            "   Runtime (s)  Memory Peak (MB)  \n",
            "0   184.044011          0.944616  \n",
            "1   223.002790          2.948337  \n",
            "2   414.240446         12.282879  \n",
            "3   417.952264         17.009860  \n"
          ]
        }
      ]
    }
  ]
}